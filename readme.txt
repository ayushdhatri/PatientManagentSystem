Hello, Spring

why use DTOS?

-> Keep the internal domain models(your database entties) hidden from the client. This protects your app internal strcutre.

-> Allow you to send or receive only the fields relevant to the client, rather than exposing all field in your domain model.

-> Request DTOs allow you to validate client input (eg. checking if fields are null or meet certain criteria)


#api-request folder :
here we will be creating different sub-folders for testing the different endpoints of a service....
-> eg. patient-service : here we will list all the api-endpoints, provided by patient-service microservice


### just curious how error is handled in spring boot

1ï¸âƒ£ Exception is thrown inside Service
2ï¸âƒ£ It travels up (bubbles) to Controller
3ï¸âƒ£ Controller does not catch it(because there is not try-catch block)
4ï¸âƒ£ Spring Framework catches it in internal layer (DispatcherServlet)
5ï¸âƒ£ It tries to resolve exception using configured resolvers
6ï¸âƒ£ It sees @ControllerAdvice handlers available
7ï¸âƒ£ Finds method matching the exception type
8ï¸âƒ£ Calls your handler to convert it â†’ JSON Response


@Validation : It checks for each field and if its blank it will throw proper message and response with error code 400

Whenever we want that based on request type in controller, like get, put, post, patch, we want to validate the fields with incoming request
then in that case we can use the validators

When you use @Validated(Default.class) we simply means that Default.class only(which includes @NotBlank, @NotNULL...etc)
When you use @Validated(Default.class, CreatePatientValidation.class) it means validated it for both default class and alos for CreatePatientValidation.class)

So basically you have to assign groups to each field, and then use in the controller for grouping simple.
Note : you also need to remove the validation from both the getter setter of that variable



Adding Swagger : (bit complext to add but just add dependecy proper veresion which matches the compatibilty with spring boot)
Once Added and build successfully you need to visit the URL : http:://localhost:4000/v3/api-docs
// make hit to url copy the whole json
// open the swigger editor
//


#Docker :

Simplify building and deployment of apps using lightweight containers
It resolves the dependency issue  and compatibility issues
It consist of a single thing(artifact) needed to run the app on any machine

How to push particular microservice to docker container and use it?

#Step1 :
    Create a file name DockerFile
    Whatever image that we stored inside the container goes inside the folder structure is var/lib/docker
    ok got it, basically means there overrall extra memory is used by docker as space consiting of all the enviorment requried to run the application
    ->A Dockerfile is a script that tells Docker: â€œHow to build this application into a container imageâ€



Absolutely â€” let's deeply break down each line of your Dockerfile and understand what exactly is happening inside Docker ğŸ‘‡

ğŸ”· Stage 1: Builder Stage
This stage compiles the Spring Boot application into a .jar file.

dockerfile
Copy code
FROM maven:3.9.9-eclipse-temurin-21 AS builder
âœ” What this does:
Selects a base image with Maven + Java 21 installed.

Names this stage builder so it can be referred later.

This environment is heavy because it contains:

Linux OS

JDK 21

Maven + all build tools

Purpose: Build the app but not included in final container.

dockerfile
Copy code
WORKDIR /app
âœ” What this does:
Creates & switches into a folder inside container /app

All next operations happen inside /app

Think of it as cd /app.

dockerfile
Copy code
COPY pom.xml .
âœ” What this does:
Copies only pom.xml into container

Why only pom.xml first?
â†’ To cache dependency downloads in next step
â†’ So that future builds may skip downloading Maven dependencies if only source code changed

dockerfile
Copy code
RUN mvn dependency:go-offline -B
âœ” What this does:
Downloads all dependencies defined in pom.xml

-B means batch mode: no user interaction required

âš¡ Benefit: Faster builds later
Docker caching will reuse this layer unless dependencies change.

dockerfile
Copy code
COPY src ./src
âœ” What this does:
Copy your Java code into the container

Now Maven can compile it

dockerfile
Copy code
RUN mvn clean package
âœ” What this does:
Runs full Maven build

Compiles code and creates .jar under:

ğŸ“Œ /app/target/patient-service-0.0.1-SNAPSHOT.jar

Also cleans old builds

After this step â†’ your backend is fully compiled inside container

ğŸ§  Recap Stage 1
Purpose	Output
Build Java app	.jar file ready inside container
Heavy tools included (Maven/JDK)	âŒ Will NOT be in final image

ğŸŸ¦ Stage 2: Runner Stage
This stage runs the already-built JAR.

dockerfile
Copy code
FROM openjdk:21-jdk AS runner
âœ” What this does:
Starts a fresh lightweight JDK 21 image

Much smaller than Maven image

Will run the compiled .jar

â¡ This is what becomes your production image.

dockerfile
Copy code
WORKDIR /app
Same purpose: working directory = /app

dockerfile
Copy code
COPY --from=builder ./app/target/patient-service-0.0.1-SNAPSHOT.jar ./app.jar
âœ” What this does:
Copies JAR from builder stage â†’ into /app directory of final image

Only the final .jar is copied â€” no source code, no Maven, nothing extra

This is key to security & small size ğŸ‘Œ

dockerfile
Copy code
EXPOSE 4000
âœ” What this does:
Documents that application listens on port 4000

Does NOT open host port automatically

You will still need when running:

sh
Copy code
docker run -p 4000:4000 <image-name>
dockerfile
Copy code
ENTRYPOINT ["java", "-jar", "app.jar"]
âœ” What this does:
Defines command that starts your Spring Boot app when container runs

ENTRYPOINT makes sure Java always runs, even if someone appends arguments

ğŸ¯ Final Outcome
Stage	Contains	Used in Final Image?
builder	Maven + source + compiled JAR	âŒ No
runner	Only runtime JDK + app.jar	âœ” Yes

â¡ This means:

Smaller image

Faster deploy

Cleaner & more secure (no build tools exposed)

ğŸ“Œ What to Run
sh
Copy code
docker build -t patient-service .
docker run -p 4000:4000 patient-service
Then backend available at:
ğŸ‘‰ http://localhost:4000

ğŸŒŸ Why This Is Best Practice?
âœ” Multi-stage build = optimal image size
âœ” Caches Maven dependencies
âœ” Builds in a clean environment
âœ” Final image has only what is needed to run

If you want, I can also explain:

ğŸ”¹ How Docker layers work
ğŸ”¹ How to reduce image size further (e.g., JRE instead of JDK)
ğŸ”¹ How to add environment variables (DB credentials)
ğŸ”¹ Docker Compose to run backend + Postgres together


# gRPC :
gRPC Internal Working â€” Full Deep Dive
Below is the exact flow for a Unary RPC Call, from client function call â†’ server function execution â†’ response.

ğŸ”· 0ï¸âƒ£ Before any request â€” Setup (One time)
A. Developer defines API in .proto
Compiler (protoc) generates code:

Client Stub: Looks like a local function call

Server Skeleton: Dispatches to real implementation

B. TCP + HTTP/2 connection established
ALPN negotiation (TLS handshake if secure)

One connection can handle many parallel RPCs (HTTP/2 streams)

â–¶ï¸ 1ï¸âƒ£ Client Calls a gRPC Method
User calls like normal code:

java
Copy code
UserResponse res = stub.getUser(UserRequest.newBuilder().setId(10).build());
Internally:

Stub encodes the request message object â†’ Protocol Buffers (binary)

Assigns a Stream ID (HTTP/2 feature)

Creates metadata (headers) including:

:method = POST

content-type = application/grpc+proto

te = trailers

grpc-timeout, etc.

All headers compressed via HPACK (HTTP/2 header compression).

ğŸ”¶ 2ï¸âƒ£ Serialization â€” Data Conversion
How Protobuf serialization works in detail:
Message fields get converted into:

Component	Meaning
Field Number	Identifies the field (defined in .proto)
Wire Type	Specifies encoding (varint, fixed32, string, etc.)
Value	Binary encoded data

Format:

ini
Copy code
Key = (field_number << 3) | wire_type
Value = encoded field value
Example:

proto
Copy code
message UserRequest {
  int32 id = 1;
}
Becomes (binary):

Copy code
08 0A
08 = Key for field 1 (varint)
0A = value (decimal 10 encoded)

âœ” Tiny payload
âœ” Very fast encode/decode
âœ” Schema makes structure known on both sides

ğŸ“¦ 3ï¸âƒ£ Message Framing (gRPC-specific)
Each message wrapped in a gRPC Data Frame:

Byte	Meaning
1 byte	Compression flag
4 bytes (uint32)	Message length
N bytes	Protobuf serialized message

Example:

Copy code
0x00 00 00 00 02 08 0A
Now ready for network transfer.

ğŸŒ 4ï¸âƒ£ HTTP/2 Transmission
Data written into the TCP socket:

Multiplexed streams â†’ no blocking

Flow control window ensures optimal packet flow

Server receives:

Parses HTTP/2 headers

Passes data frames to gRPC library

ğŸ¢ 5ï¸âƒ£ Server Decoding + Dispatch
Flow inside the server:

HTTP/2 layer extracts message bytes

gRPC runtime removes framing

Protobuf deserializer reconstructs objects

Routed to correct service using:

Method name from header: "/UserService/GetUser"

Stream ID mapping

Finally calls your function:

java
Copy code
public UserResponse getUser(UserRequest req) {
    // business logic
}
âœ” From binary â†’ actual argument object
âœ” Type-safe call using generated code

ğŸ§® 6ï¸âƒ£ Server Response
Server performs reverse path:

Step	What happens
Business logic	Executes and creates response object
Serialize	Convert object â†’ Protobuf binary
gRPC framing	Add compression flag + length
HTTP/2	Send over same Stream ID
TCP	Over network to client

Server sends trailers after data:

Final status â†’ grpc-status: 0 (OK)

Optional grpc-message for errors

ğŸ” 7ï¸âƒ£ Client Decoding & Returning Result
Client:

Reads frames for that Stream ID

Remove gRPC framing

Deserialize Protobuf â†’ Original object

Stub returns it like a local function return

java
Copy code
return deserializedUserResponse;
âœ” No parsing JSON
âœ” No reflection-based dynamic fields
âœ” No multi-call overhead

ğŸ¥‡ Why This Whole Flow is Fast
Optimization	Benefit
Binary serialization (protobuf)	Minimal CPU + network cost
Predefined schemas	No runtime field discovery
HTTP/2 multiplexing	Avoid connection creation per request
HPACK compression	Smaller metadata â†’ lower latency
Single TCP connection	Faster round-trips
Streaming support	No polling + real-time efficiency

Latency can improve 5Ã—â€“10Ã— vs REST JSON
Bandwidth usage can drop by 50%â€“90%

ğŸ”„ Visual Summary
pgsql
Copy code
Client Stub
   â†“ (Protobuf serialization)
gRPC Framing
   â†“
HTTP/2 Stream
   â†“ (TCP packets)
Server HTTP/2
   â†“
gRPC Runtime
   â†“ (Protobuf deserialization)
Server Handler (Function invoked)
   â†“ (Response generated)
Reverse path back to Client



##when you should use kafka and when you should use grpc##########

-> gRPC : 1-1 microservice communication. when you need to an immediate response (synchronous)

-> Kafka : 1 : m (1 to many) microservice communication , do not need an immediate response (asynchronous)

#### Introduce Authentication and Authorization ###########











































